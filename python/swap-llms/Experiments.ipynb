{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f589e4-de41-40ea-8091-14a2db6a1dd3",
   "metadata": {},
   "source": [
    "# Swap LLMs and Validate App Performance\n",
    "\n",
    "You're an AI Engineer recently tasked with switching to the latest LLM release? Follow this tutorial to ship your changes **confidently** with Literal AI ! ðŸš€\n",
    "\n",
    "\n",
    "To show the experiments flow, we will go through these steps:\n",
    "- [Setup](#setup)\n",
    "- [Get dataset](#get-dataset)\n",
    "- [Run experiment against `gpt-4o-mini`](#run-experiment)\n",
    "   - [Load embedding model](#load-embedding-model)\n",
    "   - [Create experiment](#create-experiment)\n",
    "   - [Test each sample](#test-each-sample)\n",
    "\n",
    "We deployed a chatbot to answer user questions about the Chainlit framework. In short, it's a **R**etrieval **A**ugmented **G**eneration (RAG) application, with access to the Chainlit [documentation](https://docs.chainlit.io/) and [cookbooks](https://github.com/Chainlit/cookbook). \n",
    "\n",
    "\n",
    "<div style=\"flex-direction: column; display: flex; justify-content: center; align-items: center\">\n",
    "    <figure style=\"margin-left: 350px; margin-right: 100px\">\n",
    "        <center>\n",
    "            <figcaption><b>RAG chatbot in Chainlit</b></figcaption>\n",
    "        </center>\n",
    "        <br/>\n",
    "        <img src=\"images/chainlit-rag-copilot.jpg\" width=\"440\"/>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <center>\n",
    "            <figcaption><b>Thread details in Literal AI</b></figcaption>\n",
    "        </center>\n",
    "        <br/>\n",
    "        <img src=\"images/thread-details-rag.jpg\" width=\"525\"/>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "We focus on the iteration loop to validate a parameter change we make on the application.<br/>\n",
    "For instance, OpenAI released `gpt-4o-mini` yesterday. How do we confidently switch from `gpt-4o` to that cheaper version?\n",
    "\n",
    "In this notebook, we will show the flow by checking how dissimilar `gpt-4o-mini` answers are from expected ground truths. <br/>\n",
    "In a real-world scenario, you would have a few metrics you check against before shipping the change: context relevancy, answer similarity, latency, cost, latency, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9e10a-b577-4e92-a894-7aa722b4a837",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup\n",
    "\n",
    "Get your [API key](https://docs.getliteral.ai/get-started/installation#how-to-get-my-api-key) and connect to Literal AI!\n",
    "\n",
    "The below cell will prompt you for your `LITERAL_API_KEY` and create a `LiteralClient` which we will use to get our dataset and push the result of our experiments ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e4a333-b121-44f1-9f2d-1ea7ab6ad018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from literalai import LiteralClient\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LITERAL_API_KEY\")\n",
    "\n",
    "literal_client = LiteralClient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e9b80b6-2e9f-45d5-94c2-be5573f07c3d",
   "metadata": {},
   "source": [
    "<a id=\"get-dataset\"></a>\n",
    "## Get dataset\n",
    "\n",
    "Here's what our dataset looks like on Literal AI. It contains:\n",
    "1. the questions in the **Input** column\n",
    "2. the answers (ground truths) in the **Output** column\n",
    "3. the intermediate steps taken by our RAG agent in the dashed box\n",
    "\n",
    "We will fetch the whole dataset, but focus on `input` and `output` for this tutorial.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/faq-dataset.jpg\" width=80%/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d241926-8b7b-44ed-a225-16ecd7340795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset = 5\n"
     ]
    }
   ],
   "source": [
    "# Adapt below to your own dataset\n",
    "dataset = literal_client.api.get_dataset(name=\"Test dataset to ship RAG\")\n",
    "\n",
    "print(f\"Number of samples in dataset = {len(dataset.items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427368f-9a0f-40d6-8b15-4e920ed4a035",
   "metadata": {},
   "source": [
    "<a id=\"run-experiment\"></a>\n",
    "## Run experiment against `gpt-4o-mini`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72e0f2-24d9-4a42-9689-73ee88b641f6",
   "metadata": {},
   "source": [
    "<a id=\"load-embedding-model\"></a>\n",
    "### Load embedding model\n",
    "\n",
    "We compute Answer Semantic Similarity using [gte-base-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) hosted on HuggingFace ðŸ¤—ðŸ¤—\n",
    "\n",
    "Check out the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to pick the right embedding model for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466263df-e447-4910-9c55-fa3bd5175640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995fecf-981c-48cf-9224-8458779d1fe0",
   "metadata": {},
   "source": [
    "<a id=\"create-experiment\"></a>\n",
    "### Create experiment\n",
    "\n",
    "Let's start with creating a new experiment for our dataset. \n",
    "\n",
    "It's good practice to provide a meaningful name summarizing the changes you made. <br/>\n",
    "In the `params` field, you can pass the exhaustive list of parameters that characterize the experiment you are about to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5405d9b9-8c2a-4a2d-a067-8ff5475de4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(\n",
    "    name=\"Trial with gpt-4o-mini\",\n",
    "    params={ \n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"type\": \"output similarity\", \n",
    "        \"embedding-model\": \"Alibaba-NLP/gte-base-en-v1.5\", \n",
    "        \"commit\": \"830a6d1ee79e395e9cdcc487a6ec923887c29713\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871192d-61de-4439-be85-5783d91d95a6",
   "metadata": {},
   "source": [
    "<a id=\"test-each-sample\"></a>\n",
    "### Test each sample\n",
    "\n",
    "Simply loop on the dataset and for each entry:\n",
    "- send the `question` to the locally modified version of our application (using `gpt-4o-mini`)\n",
    "- compute the cosine similarity between **ground truth** and the **reached answer**\n",
    "- log the resulting value as a score on our experiment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5141418-a39e-4c89-952e-6a65dffaad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished and all 5 logged on Literal AI! ðŸŽ‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "for item in tqdm(dataset.items):\n",
    "    question = item.input[\"content\"][\"args\"][0]\n",
    "\n",
    "    # Reached answer - based on locally modified version of the RAG application\n",
    "    response = requests.get(f\"http://localhost/app/{question}\")\n",
    "    answer = response.json()[\"answer\"]\n",
    "    answer_embedding = model.encode(answer)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth = item.expected_output[\"content\"]\n",
    "    ground_truth_embedding = model.encode(ground_truth)\n",
    "\n",
    "    similarity = float(cos_sim(answer_embedding, ground_truth_embedding))\n",
    "    \n",
    "    experiment.log({\n",
    "        \"datasetItemId\": item.id,\n",
    "        \"scores\": [ {\n",
    "            \"name\": \"Answer similarity\",\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": similarity\n",
    "        } ],\n",
    "        \"input\": { \"question\": question },\n",
    "        \"output\": { \"answer\": answer }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b4e6b-e43c-47df-b8a9-050ce73a2751",
   "metadata": {},
   "source": [
    "## Compare experiments on Literal AI ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "Here is the comparison between the `gpt-4o` and `gpt-4o-mini` experiments on Literal AI!\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/experiments-comparison.jpg\" width=80%/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29ade9-ec83-44a0-abe4-09779d2a2943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
