{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64364a97-0b72-4faf-986e-428e01b03607",
   "metadata": {},
   "source": [
    "# LangGraph example\n",
    "\n",
    "In this guide, we will see how to integrate Literal AI in a LangGraph workflow.\n",
    "\n",
    "- [Necessary imports](#imports)\n",
    "- [Define tools](#define-available-tools)\n",
    "- [Agent logic](#agent-logic)\n",
    "- [Run agent](#run-agent)\n",
    "\n",
    "<a id=\"imports\"></a>\n",
    "## Necessary imports\n",
    "\n",
    "Make sure to define the `LITERAL_API_KEY`, `OPENAI_API_KEY` and `TAVILY_API_KEY` in your `.env`.\n",
    "\n",
    "If you have a prompt template, check https://docs.literalai.com/guides/prompt-management#convert-to-langchain-chat-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cba71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "import os\n",
    "from literalai import LiteralClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf4ba8",
   "metadata": {},
   "source": [
    "# Check environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664be3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "# print(os.getenv(\"LITERAL_API_KEY\"))\n",
    "# print(os.getenv(\"TAVILY_API_KEY\"))\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b540c-a99c-46e3-86d7-d68f6bf59f80",
   "metadata": {},
   "source": [
    "<a id=\"define-available-tools\"></a>\n",
    "## Define available tools\n",
    "\n",
    "We will use Tavily as a search tool. `tools` is a list of available tools  (here, we only have one tool, the TavilySearchResults tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb0ac3b-1a27-4a2b-b4fd-db4da8519bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "literalai_client = LiteralClient()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Define the tool (TavilySearchResults tool to search the web)\n",
    "tool = TavilySearchResults(max_results=2, k=2)\n",
    "tools = [tool]\n",
    "\n",
    "# Define the LLM (chatGPT 4o-mini)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c21e18-f6e4-4485-92c6-15b3e08f4292",
   "metadata": {},
   "source": [
    "<a id=\"agent-logic\"></a>\n",
    "## Agent logic\n",
    "\n",
    "For the agent logic, we simply repeat the following pattern (max. 5 times):\n",
    "- ask the user question to the LLM, making the tools available\n",
    "- execute tools if LLM asks for it, otherwise return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33683f5e-c818-4e46-8804-4cfcce066880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chatbot logic\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# Add the chatbot node to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Add the tool node to the graph\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8b912-059c-4f61-b988-ff8e18ba0363",
   "metadata": {},
   "source": [
    "<a id=\"run-agent\"></a>\n",
    "## Run agent against a question\n",
    "\n",
    "The agent has a pre-set user question (What is the weather in Paris?). It is run in a separate thread to log the output in literal, and then prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788fab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Paris is sunny with a temperature of 13.5°C (56.3°F). The wind is coming from the east at 7.9 kph (4.9 mph), and the humidity level is 69%. There is no precipitation expected, and visibility is good at 10 km.\n",
      "\n",
      "For more details, you can check out the full forecast [here](https://www.weatherapi.com/) or see the hourly forecast [here](https://www.timeanddate.com/weather/france/paris/hourly).\n"
     ]
    }
   ],
   "source": [
    "# wait for user input and then run the graph\n",
    "with literalai_client.thread(name=\"Weather in Paris\") as thread:\n",
    "    user_input = \"What is the weather in Paris?\"\n",
    "    cb = literalai_client.langchain_callback()\n",
    "    res = graph.invoke({\"messages\": [HumanMessage(content=user_input)]}, config=RunnableConfig(callbacks=[cb]))\n",
    "    print(res[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2795c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT\n",
    "\n",
    "def run_agent(input: str):\n",
    "    # Use the Runnable\n",
    "    cb = literalai_client.langchain_callback()\n",
    "    final_state = graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=input)]},\n",
    "        config={\"callbacks\": [cb]}\n",
    "    )\n",
    "    return final_state\n",
    "\n",
    "\n",
    "\n",
    "experiment = literalai_client.api.create_experiment(\n",
    "    name=\"LangGraph\", params=[]  # optional\n",
    ")\n",
    "\n",
    "def score_output(input, output, expected_output=None):\n",
    "    # Faking the scoring\n",
    "    return [{\"name\": \"context_relevancy\", \"type\": \"AI\", \"value\": 0.6}]\n",
    "\n",
    "\n",
    "@literalai_client.experiment_item_run\n",
    "def run_and_eval(input, expected_output=None):\n",
    "    lc_output = run_agent(input)\n",
    "    output = cb.process_content(lc_output)\n",
    "    experiment_item = {\n",
    "      \"scores\": score_output(input, output, expected_output),\n",
    "      \"input\": {\"question\": input},\n",
    "      \"output\": {\"answer\": output}\n",
    "    }\n",
    "    experiment.log(experiment_item)\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(inputs):\n",
    "    for input in inputs:\n",
    "        run_and_eval(input)\n",
    "\n",
    "\n",
    "\n",
    "run_experiment([\"What is the weather in SF?\", \"What is the weather in Paris?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e3ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
